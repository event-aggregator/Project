#!/usr/bin/env python
# coding: utf-8

import pandas as pd
import re
import math
import nltk
import pymorphy2
import numpy as np
from collections import Counter
from nltk.corpus import stopwords
from nltk import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer
# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
# Plots
import pyLDAvis
import pyLDAvis.gensim  # don't skip this

import json, requests
import pandas as pd
import numpy as np
from pandas.io.json import json_normalize

def timepad(url):
    data = pd.DataFrame(index=[], columns=[]).fillna(0) #Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿ÑƒÑÑ‚Ğ¾Ğ³Ğ¾ dataframe
    skip=0 #ĞœĞ¾Ğ¶Ğ½Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ 100 ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ‚ÑŒ, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ñ†Ğ¸ĞºĞ», Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚:
    while True:
        response = requests.get(url) #ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ñ ÑĞµÑ€Ğ²ĞµÑ€Ğ°
        data_add=json_normalize(pd.DataFrame(response.json())['values'])#ĞÑ‚Ğ²ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€Ğ¸Ğ²Ğ¾Ğ¹, Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ² Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ values, Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ values, Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ğ¼ json Ğ² dataframe 
        if data_add.empty:
            break
        data=(pd.concat([data, data_add], ignore_index=True,sort=False))#Ğ¡Ğ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ñ‚Ğ°Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†
        skip+=100
        url=url.replace('skip='+str(skip-100), 'skip='+str(skip))#Ğ˜Ğ·Ğ¼ĞµĞ½ÑÑ Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ 100 ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†
    #ĞŸĞµÑ€ĞµĞ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ğ¾Ğ³Ğ¾:
    data.drop(['categories','description_short','moderation_status','organization.id','poster_image.default_url','organization.logo_image.uploadcare_url','organization.logo_image.default_url','organization.subdomain','organization.description_html', 'organization.url','location.coordinates'], axis=1, inplace=True)
    data=data.rename({'description_html': 'description','location.country' :'country','url': 'link', 'poster_image.uploadcare_url': 'image', 'location.city': 'city', 'location.address': 'address', 'organization.name': 'group_name'}, axis=1)
    date_time=data['starts_at'].str.split('T',expand=True) #ĞŸÑ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ´Ğ°Ñ‚Ñ‹ Ğ² Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´:
    date_time.columns=['date','time']
    date_time.time=pd.Series(date_time['time']).str.replace(':00+0300', ' ',regex=False)
    data=(pd.concat([data, date_time],axis=1))
    data = data[data.country == 'Ğ Ğ¾ÑÑĞ¸Ñ'] #ĞÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑ€Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ Ğ¾ÑÑĞ¸Ğ¸
    data.drop(['starts_at','country'],axis=1,inplace=True) #Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº
    data.loc[data.city == 'Ğ‘ĞµĞ· Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°', 'city'] = 'ĞĞ½Ğ»Ğ°Ğ¹Ğ½' #Ğ’ÑÑ‘, Ñ‡Ñ‚Ğ¾ Ğ±ĞµĞ· Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ° - Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¼ĞµÑ€Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ
    return data

def meetup(url):
    data = pd.DataFrame(index=[], columns=[]).fillna(0)
    key=['791b24a3e1670f2651183c1c2e4c5c', '3d7f5d5049b117e74666395f306e43','264e2c412f705c562a2f304df55769','573f6f2839355971c476b6a294d1025','4b21301a37523464f4d395817dd36']
    city=['Ğ¡Ğ°Ğ½ĞºÑ‚-ĞŸĞµÑ‚ĞµÑ€Ğ±ÑƒÑ€Ğ³','ĞœĞ¾ÑĞºĞ²Ğ°','ĞĞ¾Ğ²Ğ¾ÑĞ¸Ğ±Ğ¸Ñ€ÑĞº','Ğ•ĞºĞ°Ñ‚ĞµÑ€Ğ¸Ğ½Ğ±ÑƒÑ€Ğ³','ĞĞ¸Ğ¶Ğ½Ğ¸Ğ¹ ĞĞ¾Ğ²Ğ³Ğ¾Ñ€Ğ¾Ğ´']
    for i in range(len(key)): 
        response = requests.get(url, params={'key': key[i]}) #ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ñ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¼ Ğ¸Ğ· Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²
        data_add=json.dumps(response.json()) #ĞŸÑ€ĞµĞ¾Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ json Ğ² ÑÑ‚Ñ€Ğ¾ĞºÑƒ
        data_add="{"+data_add[data_add.find('}')+2:] #ĞÑ‚Ğ²ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ĞµÑÑŒĞ¼Ğ° Ğ½ĞµĞ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ñ‚Ğ°Ğº Ğ½Ğ°Ğ´Ğ¾ :D
        data_add=json_normalize(pd.DataFrame(json.loads(data_add))['events']) #ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² json, Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ² Ğ´Ğ°Ñ‚Ğ°Ñ„Ñ€ĞµĞ¹Ğ¼, Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼
        data_add=data_add.rename({'venue.city':'city'}, axis=1) #ĞŸĞµÑ€ĞµĞ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ±ĞµĞ· Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ±Ñ‹Ğ»Ğ° 
        data_add.city=city[i] 
        data=(pd.concat([data, data_add], ignore_index=True,sort=False)) #Ğ¡Ğ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ñ‚Ğ°Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ¼
    #Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ğ¾Ğµ, Ğ¿ĞµÑ€ĞµĞ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ĞµĞ¼:
    data=data[['id','name','local_date', 'local_time','city','status','link', 'description', 'group.name','visibility','group.urlname','venue.address_1', 'venue.address_2','venue.name']]
    data=data.rename({'group.name': 'group_name','group.urlname':'urlname','local_date': 'date','local_time': 'time','status':'address','visibility':'image'}, axis=1)
    data.address=data.apply(lambda x: f"{x['venue.address_1']} {x['venue.address_2']} {x['venue.name']}", axis=1) #ĞŸÑ€Ğ¸Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ¶ 3 ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ñ Ğ°Ğ´Ñ€ĞµÑĞ¾Ğ¼, ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑĞ¼ Ğ¸Ñ…
    data.drop(['venue.address_1','venue.address_2','venue.name'], axis=1, inplace=True) #Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾ĞµĞ´Ğ¸Ğ½Ğ¸Ğ»Ğ¸
    data.address=pd.Series(data['address']).str.replace('nan', ' ',regex=False)
    for urlname in data.urlname: #ĞšĞ°Ñ€Ñ‚Ğ¸Ğ½Ğ¾Ñ‡ĞºĞ¸
        url_image= 'https://api.meetup.com/'+urlname+'/photos?&sign=true&photo-host=public&page=2'
        response=requests.get(url_image).json()
        if len(response)==2:
            data.loc[data.urlname==urlname,'image']=response[1]["highres_link"]
        else:
            data.drop(data[data.urlname==urlname].index,axis=0, inplace=True)
    data.drop(['urlname'], axis=1, inplace=True)
    return data

def get_dataframe():
    url_timepad='https://api.timepad.ru/v1/events?fields=description_short%2C%20description_html%2Clocation%2C%20organization&limit=100&skip=0&category_ids=452&access_statuses=public&moderation_statuses=featured%2C%20shown'
    url_meetup='https://api.meetup.com/find/upcoming_events?&sign=true&photo-host=public&topic_category=34&page=1000&allMeetups=true'
    data=(pd.concat([meetup(url_meetup),timepad(url_timepad)], ignore_index=True,sort=False))
    return data
    

def parser():
    data=get_dataframe()
    data_json=data.to_json(force_ascii=False)
    return data_json

def cities():
    city=[]
    url_timepad='https://api.timepad.ru/v1/events?fields=description_short%2C%20description_html%2Clocation%2C%20organization&limit=100&skip=0&category_ids=452&access_statuses=public&moderation_statuses=featured%2C%20shown'
    url_meetup='https://api.meetup.com/find/upcoming_events?&sign=true&photo-host=public&topic_category=34&page=1000&allMeetups=true'
    data=(pd.concat([meetup(url_meetup),timepad(url_timepad)], ignore_index=True,sort=False))
    for element in data.city:
        if element not in city:
            city.append(element)
    city.remove('ĞĞ½Ğ»Ğ°Ğ¹Ğ½')
    return city
                            
                            
df = get_dataframe()
#nltk.download('punkt')

def clean_tweet(tweet):
    if pd.isna(tweet)==False :
        tweet = re.sub('http\S+\s*', '', tweet) 
        tweet = re.sub('href\S+\s*', '', tweet)
        tweet = re.sub('www\s*', '', tweet)
        tweet = re.sub('\s\w\s', ' ', tweet)
        tweet = re.sub("\d+|-\d+", ' ',tweet)
        tweet = re.sub('<\w*\s\w+\W+\s*\w+\W+\w*\W*\w*\W+>', ' ', tweet)
        tweet = re.sub('</*\w*/*>*', ' ', tweet)
        tweet = re.sub('&\w+', ' ', tweet)
        tweet = re.sub('\W{2}', ' ', tweet)
        tweet = re.sub('\n', ' ', tweet)
        tweet = re.sub('\t', ' ', tweet) 
        tweet = re.sub('\xa0', ' ', tweet)
        tweet = re.sub('-Ğ³Ñ€Ğ°Ğ±Ğ»', ' ', tweet)
        tweet = re.sub('\[\w+\]',' ', tweet) 
        tweet = re.sub('â€¢|//|\*|=+|â†”|--|â€”â€”| â€“| â€”| -| -',' ',tweet)
        tweet = re.sub('@\S+', '', tweet)
        tweet = re.sub('style|text|align| scr |justify| Ñ | pm |gdg| sb |rte|width|font|â€”| gc |size|bgcolor|dcdcdc|rsvp|height| alt | px | src |colspan| st | spb |class| gt | lt | br | p |center|left', ' ', tweet)
        tweet = re.sub('[%s]' % re.escape("!''""#$%&()â€œâ€Â«Â»*â•°+,â„–./:;<=>âœ”âœ“Â®â„¢?@[\]^_`{|}~ğŸ“ğŸ¥³ğŸš¨ğŸ¢ğŸ°ğŸ‘ğŸ¤™ğŸ‘ŒğŸš ğŸ™†ğŸ¤©ğŸ¤ğŸ˜¶ğŸ¤ ğŸ¨ğŸ€„ğŸŒ¾ğŸš‚ğŸ§—ğŸ•ğŸ¤¡ğŸŒŸğŸƒâ™»ï¸ğŸ˜±ğŸ§ ğŸš¶ ğŸ˜‹ğŸ‘‡âœ¨ğŸ•ºğŸ’ƒğŸ¥‚ğŸ¬ğŸš©ğŸ“†â°ğŸ™‚ğŸ˜‰â˜•ğŸ§—ğŸ“¢ğŸ’¬ğŸ˜â—âœŠğŸ¿ğŸ“ŒğŸ¤—ğŸğŸŒğŸ”¥ğŸ¦¸â€â™‚ï¸ğŸ¦¸â€â™€ï¸ğŸ‘©â€ğŸš€âœ…ğŸ‘¨â€ğŸ¨ğŸ‘¨â€âœˆï¸ğŸ‘©â€âœˆï¸âš¡ğŸ”‹ğŸ“±ğŸ•µğŸ»â€â™‚ï¸ğŸ“¨ğŸ§³ğŸ¤ğŸ‘‹ğŸ¼ğŸ¡ğŸ˜ƒğŸ’ŸğŸ¥â˜ï¸ğŸ§¾â¤ï¸ğŸ’°ğŸš€ğŸ’¡ğŸ¤”ğŸ‘‰ğŸ¥’ğŸŒğŸ™â€â™‚ğŸ˜ŠğŸ˜ŒğŸ—£â˜®ğŸ’¥ğŸ ğŸ“ğŸ“"), ' ', tweet) # ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ğ¸
    return tweet 
    
df1 = df.copy()
#df1 = pd.read_excel('Data (1).xlsx')
# dropping passed columns 
df1.drop(["id", "address","date","time","link","image","city"], axis = 1, inplace = True) 

df2 = df.copy()
df2.drop(["id", "address","date","time","link","image","city"], axis = 1, inplace = True) 

for i in range(df1.shape[0]):
    if pd.isna(df1['description'][i])==True :
        df1['description'][i] = 'No description'
df1['Combined'] = df1.fillna(' ').sum(axis=1)

for i in range(df2.shape[0]):
    if pd.isna(df2['description'][i])==True :
        df2['description'][i] = 'No description'
df2['Combined'] = df2.fillna(' ').sum(axis=1)

#Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°, ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ñ†Ğ¸Ñ„Ñ€, Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ñ€ĞµĞ¿Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
df1['Combined']= df1['Combined'].map(lambda x: x.lower())
for i in range(df1.shape[0]):
    df1['Combined'][i]=str(clean_tweet(df1['Combined'][i]))
    #print('{})'.format(i) + df1['description'][i])
    #print('\n')

df1['Combined'] = df1['Combined'].astype(str)

stop_words = set(stopwords.words("english"))
stop_words1 = set(stopwords.words("russian"))

df1['Combined'] = df1['Combined'].str.split(' ').apply(lambda x: ' '.join(k for k in x if k not in stop_words))
df1['Combined'] = df1['Combined'].str.split(' ').apply(lambda x: ' '.join(k for k in x if k not in stop_words1))

#Ğ»ĞµĞ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ÑƒÑÑĞºĞ¸Ñ… ÑĞ»Ğ¾Ğ²
morph = pymorphy2.MorphAnalyzer()
df1['Combined'] = df1['Combined'].str.split(' ').apply(lambda x: ' '.join(morph.parse(tok)[0].normal_form for tok in x))
stemmer = SnowballStemmer("russian") 
df1['Combined'] = df1['Combined'].str.split(' ').apply(lambda x: ' '.join(stemmer.stem(k) for k in x ))
#ÑÑ‚ĞµĞ¼Ğ¼Ğ¸Ğ½Ğ³ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ…
stemmer = SnowballStemmer("english") 
df1['Combined'] = df1['Combined'].str.split(' ').apply(lambda x: ' '.join(stemmer.stem(k) for k in x ))

for i in range(df1.shape[0]):
    df1['Combined'][i]=str(clean_tweet(df1['Combined'][i]))

stop_words = set(stopwords.words("english"))
stop_words1 = set(stopwords.words("russian"))

df1['Combined'] = df1['Combined'].str.split(' ').apply(lambda x: ' '.join(k for k in x if k not in stop_words))
df1['Combined'] = df1['Combined'].str.split(' ').apply(lambda x: ' '.join(k for k in x if k not in stop_words1))

token = []
for i in range(df1.shape[0]):
    a=nltk.word_tokenize(df1['Combined'][i])
    token.append(nltk.word_tokenize(df1['Combined'][i]))

#TF-IDF
def compute_tfidf(corpus):
    def compute_tf(text):
        tf_text = Counter(text)
        for i in tf_text:
            tf_text[i] = 0.5 + (0.5 * (tf_text[i]/tf_text.most_common(1)[0][1]))
        return tf_text
    def compute_idf(word, corpus):
        return math.log10(1 + (len(corpus))/float(sum([1 for i in corpus if word in i])))

    documents_list = []
    for text in corpus:
        tf_idf_dictionary = {}
        computed_tf = compute_tf(text)
        for word in computed_tf:
            tf_idf_dictionary[word] = computed_tf[word] * compute_idf(word, corpus)
        documents_list.append(tf_idf_dictionary)
    return documents_list

tf_idf = compute_tfidf(token)

new_tf_idf = [{k:v for k,v in tf_dict.items() if 1 < v < 1.7} for tf_dict in tf_idf]
for i in range(len(df1['Combined'])):
    word_list = df1['Combined'][i].split()
    tmp = ''
    for word in word_list:
        if word in new_tf_idf[i]:
            tmp+= word + ' '
    df2['Combined'][i] = tmp

token = []
for i in range(df1.shape[0]):
    a=nltk.word_tokenize(df2['Combined'][i])
    token.append(nltk.word_tokenize(df2['Combined'][i]))

dct = corpora.Dictionary([df2['Combined'][0].split()])
for i in range(df1.shape[0]):
    dct.add_documents([df2['Combined'][i].split()])  
corpus = [ dct.doc2bow(doc, allow_update=True) for doc in token]

from gensim.models import LdaModel, LdaMulticore

lda_model = LdaMulticore(corpus=corpus,
                         id2word=dct,
                         random_state=100,
                         num_topics=18,
                         passes=10,
                         chunksize=1000,
                         batch=False,
                         alpha='asymmetric',
                         decay=0.5,
                         offset=64,
                         eta=None,
                         eval_every=0,
                         iterations=100,
                         gamma_threshold=0.001,
                         per_word_topics=True)

lda_model.print_topics(-1)
